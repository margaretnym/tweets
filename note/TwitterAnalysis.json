{"paragraphs":[{"title":"Prepare, run once per session","text":"z.angularBind(\"input_f\", \"\")\nz.angularBind(\"stop_words\", \"2\") \nz.angularBind(\"num_topics\", \"4\") \nz.angularBind(\"term_per_topic\", \"4\") \nz.angularBind(\"sentiment_output_path\", \"\") \nz.angularBind(\"max_iter\", \"100\") \nz.angularBind(\"exclude_retweets\", \"false\")","user":"xwj","dateUpdated":"2017-05-20T18:42:28-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1495313281674_-699570975","id":"20170304-153234_626460964","dateCreated":"2017-05-20T15:48:01-0500","dateStarted":"2017-05-20T17:31:04-0500","dateFinished":"2017-05-20T17:31:05-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12454"},{"text":"%angular\n\n<form class=\"form-inline\">\n \n  <div class=\"form-group\">\n    <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('input_f', {inputfile : inputfile,from : from_date,end : end_date, stopwords_path : stopwords_path}, '20170302-163418_715093436'); z.runParagraph('20170302-163418_715093436')\"> Load </button> tweets from\n    <input type=\"text\" class=\"form-control\" ng-init=\"inputfile='/data/projects/G-818781/data/trump/csv/2016/trump1.csv'\" placeholder=\"input data location\" ng-model=\"inputfile\">\n    between <input type=\"text\" class=\"form-control\" ng-init=\"from_date='2016-01-19 15:00:00'\" placeholder=\"2016-01-19 15:00:00\" ng-model=\"from_date\">\n    and <input type=\"text\" class=\"form-control\" ng-init=\"end_date='2018-01-19 20:00:00'\" placeholder=\"2018-01-19 20:00:00\" ng-model=\"end_date\">\n    stopwords from <input type=\"text\" class=\"form-control\" ng-init=\"stopwords_path='/data/projects/G-818781/data/trump/stopwords.txt'\" placeholder=\"input stop words location\" ng-model=\"stopwords_path\">\n    excluding retweets:<input type=\"checkbox\" class=\"form-control\" ng-init=\"false\" ng-model=\"exclude_retweets\">\n  </div>\n  <BR>\n  <div class=\"form-group\">\n     <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('stop_words', stop_words, '20170302-205317_1581000322');z.runParagraph('20170302-205317_1581000322')\"> Remove </button>\n     Top <input type=\"number\" class=\"form-control\" ng-init=\"stop_words=2\"  placeholder=3 ng-model=\"stop_words\"></input> frequent words\n   \n  </div>\n  <BR>\n  <div class=\"form-group\">\n    <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('num_topics', num_topics, '20170302-205337_768402024');\n                    z.angularBind('term_per_topic', term_per_topic, '20170302-205337_768402024');\n                    z.runParagraph('20170302-205337_768402024')\"> Run </button> topic modeling for \n    for top <input type=\"number\" class=\"form-control\" ng-init=\"num_topics=10\" placeholder=3 ng-model=\"num_topics\"></input> topcis\n    with <input type=\"number\" class=\"form-control\" ng-init=\"term_per_topic=10\" placeholder=3 ng-model=\"term_per_topic\"></input> terms per topic.\n    Maximum iterations <input type=\"number\" class=\"form-control\" ng-init=\"max_iter=100\" placeholder=100 ng-model=\"max_iter\"></input> (Note: large value will increase running time.) \n  </div>\n  <BR>\n   <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('sentiment_output_path', sentiment_output_path, '20170302-165232_309595451');z.runParagraph('20170302-165232_309595451')\"> Run </button> \n   sentiment analysis uses Stanford NLP and save result to <input type=\"text\" class=\"form-control\" ng-init=\"sentiment_output_path='sentiment/result'\" placeholder=\"sentiment/result\" ng-model=\"sentiment_output_path\"></input>\n <BR>\n  <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('senti_train_path', senti_train_path, '20170520-173801_1620357410');z.angularBind('sentiment_output_path2', sentiment_output_path2, '20170520-173801_1620357410');z.runParagraph('20170520-173801_1620357410')\"> Run </button> \n   sentiment analysis using logistic regression with training data from <input type=\"text\" class=\"form-control\" ng-init=\"senti_train_path='/data/projects/G-818781/data/trump/coded/2017_random_coded.csv'\" placeholder=\"/data/projects/G-818781/data/trump/coded/2017_random_coded.csv\" ng-model=\"senti_train_path\"></input> and save result to <input type=\"text\" class=\"form-control\" ng-init=\"sentiment_output_path2='sentiment/result'\" placeholder=\"sentiment/result\" ng-model=\"sentiment_output_path2\">\n \n  \n</form>\n","user":"xwj","dateUpdated":"2017-05-20T18:38:08-0500","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<form class=\"form-inline\">\n \n  <div class=\"form-group\">\n    <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('input_f', {inputfile : inputfile,from : from_date,end : end_date, stopwords_path : stopwords_path}, '20170302-163418_715093436'); z.runParagraph('20170302-163418_715093436')\"> Load </button> tweets from\n    <input type=\"text\" class=\"form-control\" ng-init=\"inputfile='/data/projects/G-818781/data/trump/csv/2016/trump1.csv'\" placeholder=\"input data location\" ng-model=\"inputfile\">\n    between <input type=\"text\" class=\"form-control\" ng-init=\"from_date='2016-01-19 15:00:00'\" placeholder=\"2016-01-19 15:00:00\" ng-model=\"from_date\">\n    and <input type=\"text\" class=\"form-control\" ng-init=\"end_date='2018-01-19 20:00:00'\" placeholder=\"2018-01-19 20:00:00\" ng-model=\"end_date\">\n    stopwords from <input type=\"text\" class=\"form-control\" ng-init=\"stopwords_path='/data/projects/G-818781/data/trump/stopwords.txt'\" placeholder=\"input stop words location\" ng-model=\"stopwords_path\">\n    excluding retweets:<input type=\"checkbox\" class=\"form-control\" ng-init=\"false\" ng-model=\"exclude_retweets\">\n  </div>\n  <BR>\n  <div class=\"form-group\">\n     <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('stop_words', stop_words, '20170302-205317_1581000322');z.runParagraph('20170302-205317_1581000322')\"> Remove </button>\n     Top <input type=\"number\" class=\"form-control\" ng-init=\"stop_words=2\"  placeholder=3 ng-model=\"stop_words\"></input> frequent words\n   \n  </div>\n  <BR>\n  <div class=\"form-group\">\n    <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('num_topics', num_topics, '20170302-205337_768402024');\n                    z.angularBind('term_per_topic', term_per_topic, '20170302-205337_768402024');\n                    z.runParagraph('20170302-205337_768402024')\"> Run </button> topic modeling for \n    for top <input type=\"number\" class=\"form-control\" ng-init=\"num_topics=10\" placeholder=3 ng-model=\"num_topics\"></input> topcis\n    with <input type=\"number\" class=\"form-control\" ng-init=\"term_per_topic=10\" placeholder=3 ng-model=\"term_per_topic\"></input> terms per topic.\n    Maximum iterations <input type=\"number\" class=\"form-control\" ng-init=\"max_iter=100\" placeholder=100 ng-model=\"max_iter\"></input> (Note: large value will increase running time.) \n  </div>\n  <BR>\n   <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('sentiment_output_path', sentiment_output_path, '20170302-165232_309595451');z.runParagraph('20170302-165232_309595451')\"> Run </button> \n   sentiment analysis uses Stanford NLP and save result to <input type=\"text\" class=\"form-control\" ng-init=\"sentiment_output_path='sentiment/result'\" placeholder=\"sentiment/result\" ng-model=\"sentiment_output_path\"></input>\n <BR>\n  <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('senti_train_path', senti_train_path, '20170520-173801_1620357410');z.angularBind('sentiment_output_path2', sentiment_output_path2, '20170520-173801_1620357410');z.runParagraph('20170520-173801_1620357410')\"> Run </button> \n   sentiment analysis using logistic regression with training data from <input type=\"text\" class=\"form-control\" ng-init=\"senti_train_path='/data/projects/G-818781/data/trump/coded/2017_random_coded.csv'\" placeholder=\"/data/projects/G-818781/data/trump/coded/2017_random_coded.csv\" ng-model=\"senti_train_path\"></input> and save result to <input type=\"text\" class=\"form-control\" ng-init=\"sentiment_output_path2='sentiment/result'\" placeholder=\"sentiment/result\" ng-model=\"sentiment_output_path2\">\n \n  \n</form>"}]},"apps":[],"jobName":"paragraph_1495313281677_-702264217","id":"20170302-163514_972951478","dateCreated":"2017-05-20T15:48:01-0500","dateStarted":"2017-05-20T18:38:08-0500","dateFinished":"2017-05-20T18:38:08-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12455"},{"title":"Top terms from data sets","text":"\nimport scala.collection.mutable\nimport org.apache.spark.mllib.clustering.LDA\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql._\n\n//z.angularBind(\"input_f\", \"\") \nval input_f = z.angular(\"input_f\").toString\ninput_f.substring(0, input_f.length).split(\",\")\ninput_f.substring(1, input_f.length-1).split(\",\").map(_.split(\"=\"))\nval para_map = input_f.substring(1, input_f.length-1).split(\",\").map(_.split(\"=\")).map{case Array(k, v) => (k.trim, v.trim)}.toMap\nval inputPath = para_map.getOrElse(\"inputfile\" , null)\nval from_time : String = para_map.getOrElse(\"from\", null)\nval end_time : String  =  para_map.getOrElse(\"end\", null)\nval stopWords_path=para_map.getOrElse(\"stopwords_path\", null)\n\nval remove_retweets= z.angular(\"exclude_retweets\").toString.toBoolean;\n\n//read description of each tweets\nvar tweets = spark.read.format(\"csv\").option(\"header\", true).load(inputPath)\n//val tweets = spark.read.format(\"parquet\").load(inputPath)\n        .filter('pubdate.between(from_time, end_time))\nif (remove_retweets)\n   tweets = tweets.filter(!'description.contains(\"RT @\"))\n//  .select(\"id\", \"guid\", \"link\", \"pubdate\", \"author\", \"title\", \"description\", \"source\", \"postertimezone\",\"user_id\");\n\n\n// Split each document into a sequence of terms (words)\nval stopWords = sc.textFile(stopWords_path)\nval stopWordSet = stopWords.collect.toSet\nval stopWordSetBC = sc.broadcast(stopWordSet)\nval corpus: RDD[String] = tweets.select(\"description\").filter(\"description is not null\").rdd.map(r => r(0).asInstanceOf[String])\nval tokenized: RDD[Seq[String]] =\n  corpus.map(_.toLowerCase.split(\"\\\\s\"))\n        .map(_.filter(_.length > 3)\n             .filter(_.forall(java.lang.Character.isLetter))\n             .filter( w => !stopWordSetBC.value.contains(w)))\n             \n// Show top words found in the corpus\nval termCounts: Array[(String, Long)] = tokenized.flatMap(_.map(_ -> 1L)).reduceByKey(_ + _).collect().sortBy(-_._2)\nvar b= new StringBuilder\ntermCounts.take(30).foreach { case (term, count) => b.append(s\"$term\\t$count\\n\")}\nprintln(termCounts.length + \" unique tokens from \"+ corpus.count + \" tweets.\")\nprint(s\"%table word\\t freq\\n$b\")\n","user":"xwj","dateUpdated":"2017-05-20T18:42:08-0500","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":808,"optionOpen":false}},"1":{"graph":{"mode":"multiBarChart","height":279,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport scala.collection.mutable\n\nimport org.apache.spark.mllib.clustering.LDA\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.sql.SQLContext\n\nimport org.apache.spark.sql._\n\ninput_f: String = {inputfile=/data/projects/G-818781/data/trump/csv/2016/trump1.csv, from=2016-01-19 15:00:00, end=2018-01-19 20:00:00, stopwords_path=/data/projects/G-818781/data/trump/stopwords.txt}\n\nres156: Array[String] = Array({inputfile=/data/projects/G-818781/data/trump/csv/2016/trump1.csv, \" from=2016-01-19 15:00:00\", \" end=2018-01-19 20:00:00\", \" stopwords_path=/data/projects/G-818781/data/trump/stopwords.txt}\")\n\nres157: Array[Array[String]] = Array(Array(inputfile, /data/projects/G-818781/data/trump/csv/2016/trump1.csv), Array(\" from\", 2016-01-19 15:00:00), Array(\" end\", 2018-01-19 20:00:00), Array(\" stopwords_path\", /data/projects/G-818781/data/trump/stopwords.txt))\n\npara_map: scala.collection.immutable.Map[String,String] = Map(inputfile -> /data/projects/G-818781/data/trump/csv/2016/trump1.csv, from -> 2016-01-19 15:00:00, end -> 2018-01-19 20:00:00, stopwords_path -> /data/projects/G-818781/data/trump/stopwords.txt)\n\ninputPath: String = /data/projects/G-818781/data/trump/csv/2016/trump1.csv\n\nfrom_time: String = 2016-01-19 15:00:00\n\nend_time: String = 2018-01-19 20:00:00\n\nstopWords_path: String = /data/projects/G-818781/data/trump/stopwords.txt\n\nremove_retweets: Boolean = true\n\ntweets: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, guid: string ... 17 more fields]\n\nstopWords: org.apache.spark.rdd.RDD[String] = /data/projects/G-818781/data/trump/stopwords.txt MapPartitionsRDD[1993] at textFile at <console>:294\nstopWordSet: scala.collection.immutable.Set[String] = Set(serious, latterly, down, side, moreover, please, ourselves, behind, for, find, further, mill, due, any, wherein, across, twenty, name, this, in, move, itse\", have, your, off, once, are, is, his, why, too, among, everyone, show, empty, already, nobody, less, am, hence, system, than, four, fire, anyhow, three, whereby, con, twelve, throughout, but, whether, below, co, mine, becomes, eleven, what, would, although, elsewhere, another, front, if, hereby, own, neither, bottom, up, etc, so, our, per, therein, must, beforehand, keep, do, all, him, had, somehow, re, onto, nor, every, herein, full, before, afterwards, somewhere, whither, else, namely, us, it, whereupon, two, thence, a, herse\", sometimes, became, though, within, as, because...\nstopWordSetBC: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] = Broadcast(458)\n\ncorpus: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1998] at map at <console>:298\n\ntokenized: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[2000] at map at <console>:312\ntermCounts: Array[(String, Long)] = Array((trump,27406), (donald,9831), (clinton,3060), (hillary,2918), (like,1527), (just,1408), (president,1300), (says,1248), (vote,1133), (campaign,1096), (people,950), (want,800), (bernie,706), (white,694), (america,691), (think,684), (make,661), (going,642), (wants,638), (sanders,636), (does,621), (endorses,615), (support,578), (news,577), (extraordinary,572), (said,571), (great,565), (need,556), (know,553), (supporters,533), (national,516), (republican,495), (million,460), (poll,437), (rifle,435), (good,432), (time,410), (right,403), (video,397), (voting,397), (really,396), (free,371), (called,371), (wins,368), (party,363), (presidential,362), (tells,354), (house,348), (change,341), (views,341), (american,336), (love,335), (guns,332), (media,330), ...\nb: StringBuilder =\n20130 unique tokens from 44015 tweets.\n"},{"type":"TABLE","data":"word\t freq\ntrump\t27406\ndonald\t9831\nclinton\t3060\nhillary\t2918\nlike\t1527\njust\t1408\npresident\t1300\nsays\t1248\nvote\t1133\ncampaign\t1096\npeople\t950\nwant\t800\nbernie\t706\nwhite\t694\namerica\t691\nthink\t684\nmake\t661\ngoing\t642\nwants\t638\nsanders\t636\ndoes\t621\nendorses\t615\nsupport\t578\nnews\t577\nextraordinary\t572\nsaid\t571\ngreat\t565\nneed\t556\nknow\t553\nsupporters\t533\n"}]},"apps":[],"jobName":"paragraph_1495313281678_-701109971","id":"20170302-163418_715093436","dateCreated":"2017-05-20T15:48:01-0500","dateStarted":"2017-05-20T18:08:54-0500","dateFinished":"2017-05-20T18:09:11-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12456"},{"title":"Top terms after preprocessing","text":"\n//Preprocessing the documents \n//   vocabArray: Chosen vocab (removing common terms)\n\n//z.angularBind(\"stop_words\", \"20\") \nval numStopwords = z.angular(\"stop_words\").toString.toDouble.toInt\n\n\n\nval vocabArray: Array[String] =\n  termCounts.takeRight(termCounts.size - numStopwords).map(_._1)\n\nprintln(vocabArray.length + \" tokens are used\")\n\n//   vocab: Map term -> term index\nval vocab: Map[String, Int] = vocabArray.zipWithIndex.toMap\n\n// Convert documents into term count vectors\nval documents: RDD[(Long, Vector)] =\n  tokenized.zipWithIndex.map { case (tokens, id) =>\n    val counts = new mutable.HashMap[Int, Double]()\n    tokens.foreach { term =>\n      if (vocab.contains(term)) {\n        val idx = vocab(term)\n        counts(idx) = counts.getOrElse(idx, 0.0) + 1.0\n      }\n    }\n    (id, Vectors.sparse(vocab.size, counts.toSeq))\n  }\n  \nvar b2= new StringBuilder\ntermCounts.take(numStopwords+30).takeRight(30).foreach { case (term, count) => b2.append(s\"$term\\t$count\\n\")}\nprint(s\"%table word\\t freq\\n$b2\") ","user":"xwj","dateUpdated":"2017-05-20T18:42:06-0500","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":828,"optionOpen":false}},"1":{"graph":{"mode":"multiBarChart","height":348,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nnumStopwords: Int = 2\nvocabArray: Array[String] = Array(hillary, clinton, just, people, like, vote, president, says, campaign, endorses, said, wants, best, want, make, america, great, bernie, white, going, getting, think, sanders, supporters, poll, know, support, does, death, need, called, endorsed, news, guns, voting, live, right, time, tells, obama, good, million, national, media, republican, american, free, voters, women, love, watch, income, speech, candidate, sucking, tattoo, probably, super, video, presidential, really, americans, party, stop, returns, paid, shows, daughter, real, house, welcome, maybe, admits, wore, muslim, evil, years, better, believe, muslims, thing, sexy, gets, calls, list, rifle, watched, showed, world, rally, plane, political, country, today, endorsement, raise, look, thank, extr...24539 tokens are used\nvocab: Map[String,Int] = Map(quotient -> 20276, incident -> 4616, misfire -> 4606, brink -> 17450, comply -> 22679, breaks -> 900, sandwedge -> 16337, mesmo -> 13641, mafioso -> 19758, subreddit -> 21353, sneezed -> 8717, forgotten -> 7074, insinuates -> 13194, precious -> 5995, leer -> 8090, mario -> 7959, compliment -> 4069, dynasties -> 19902, inflammatory -> 1746, ascension -> 20368, sasuke -> 17809, phénomène -> 9578, tripe -> 18667, embedded -> 15209, respecting -> 6150, rascist -> 20403, lithuanian -> 20296, ejekam -> 24439, europeisk -> 12940, accosted -> 17338, lover -> 4389, whenyou -> 17095, malignant -> 10636, elit -> 8456, speaker -> 2858, esempio -> 14019, terrible -> 1861, lion -> 3320, pres -> 1112, kalem -> 12723, votaría -> 16457, sollte -> 7573, rate -> 1636, pepper -...\ndocuments: org.apache.spark.rdd.RDD[(Long, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[21] at map at <console>:78\n\nb2: StringBuilder =\n"},{"type":"TABLE","data":"word\t freq\nhillary\t6867\nclinton\t6792\njust\t4962\npeople\t3271\nlike\t3195\nvote\t2878\npresident\t2707\nsays\t2663\ncampaign\t2289\nendorses\t1893\nsaid\t1866\nwants\t1700\nbest\t1643\nwant\t1634\nmake\t1583\namerica\t1568\ngreat\t1552\nbernie\t1505\nwhite\t1435\ngoing\t1427\ngetting\t1411\nthink\t1372\nsanders\t1313\nsupporters\t1309\npoll\t1302\nknow\t1287\nsupport\t1286\ndoes\t1257\ndeath\t1228\nneed\t1205\n"}]},"apps":[],"jobName":"paragraph_1495313281678_-701109971","id":"20170302-205317_1581000322","dateCreated":"2017-05-20T15:48:01-0500","dateStarted":"2017-05-20T16:10:12-0500","dateFinished":"2017-05-20T16:10:18-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12457"},{"title":"LDA Topic Modeling  ","text":"// Set LDA parameters\nimport org.apache.spark.sql.types._\n\n//z.angularBind(\"num_topics\", \"10\") \n//z.angularBind(\"term_per_topic\", \"10\") \nval numTopics = z.angular(\"num_topics\").toString.toDouble.toInt\nval tpt =  z.angular(\"term_per_topic\").toString.toDouble.toInt\nval maxIter =  z.angular(\"max_iter\").toString.toDouble.toInt\n\n\nval lda = new LDA().setK(numTopics).setMaxIterations(maxIter)\nval ldaModel = lda.run(documents)\nval topicIndices = ldaModel.describeTopics(maxTermsPerTopic = tpt)\nval topics = topicIndices.zipWithIndex.flatMap { case((terms, termWeights), id) => \n    terms.zip(termWeights).map { case (term, weight) =>\n       Row(id, vocabArray(term.toInt), weight )\n    }\n}\nval schema = StructType(\n  StructField(\"topic\", IntegerType, false) ::\n  StructField(\"word\", StringType, false) ::\n  StructField(\"Weight\", DoubleType,false) :: Nil)\n  \nval df =sqlContext.createDataFrame(sc.parallelize(topics), schema)\n\ndf.registerTempTable(\"result\");\nz.show(df)","user":"xwj","dateUpdated":"2017-05-20T18:42:02-0500","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":808,"optionOpen":false,"setting":{"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"topic","index":0,"aggr":"sum"}],"groups":[{"name":"word","index":1,"aggr":"sum"}],"values":[{"name":"Weight","index":2,"aggr":"sum"}]},"helium":{}},"1":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"topic","index":0,"aggr":"sum"}],"groups":[{"name":"word","index":1,"aggr":"sum"}],"values":[{"name":"Weight","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.types._\n\nnumTopics: Int = 4\n\ntpt: Int = 4\n\nmaxIter: Int = 70\n\nlda: org.apache.spark.mllib.clustering.LDA = org.apache.spark.mllib.clustering.LDA@210b1840\n\nldaModel: org.apache.spark.mllib.clustering.LDAModel = org.apache.spark.mllib.clustering.DistributedLDAModel@1f94632a\n\ntopicIndices: Array[(Array[Int], Array[Double])] = Array((Array(1, 0, 2, 4),Array(0.016892994616120736, 0.016734787251322034, 0.011451257973680915, 0.007043307604870002)), (Array(0, 1, 2, 12),Array(0.01353471716567433, 0.012542930160999563, 0.009912221074686393, 0.0095036199331644)), (Array(1, 0, 2, 4),Array(0.014659066590253695, 0.01255586937409808, 0.010078376563998265, 0.0077900180098797005)), (Array(0, 1, 2, 3),Array(0.018717746582732057, 0.01677662526301099, 0.013029174763779383, 0.010207181931985578)))\n\ntopics: Array[org.apache.spark.sql.Row] = Array([0,clinton,0.016892994616120736], [0,hillary,0.016734787251322034], [0,just,0.011451257973680915], [0,like,0.007043307604870002], [1,hillary,0.01353471716567433], [1,clinton,0.012542930160999563], [1,just,0.009912221074686393], [1,best,0.0095036199331644], [2,clinton,0.014659066590253695], [2,hillary,0.01255586937409808], [2,just,0.010078376563998265], [2,like,0.0077900180098797005], [3,hillary,0.018717746582732057], [3,clinton,0.01677662526301099], [3,just,0.013029174763779383], [3,people,0.010207181931985578])\n\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(topic,IntegerType,false), StructField(word,StringType,false), StructField(Weight,DoubleType,false))\n\ndf: org.apache.spark.sql.DataFrame = [topic: int, word: string ... 1 more field]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"},{"type":"TABLE","data":"topic\tword\tWeight\n0\tclinton\t0.016892994616120736\n0\thillary\t0.016734787251322034\n0\tjust\t0.011451257973680915\n0\tlike\t0.007043307604870002\n1\thillary\t0.01353471716567433\n1\tclinton\t0.012542930160999563\n1\tjust\t0.009912221074686393\n1\tbest\t0.0095036199331644\n2\tclinton\t0.014659066590253695\n2\thillary\t0.01255586937409808\n2\tjust\t0.010078376563998265\n2\tlike\t0.0077900180098797005\n3\thillary\t0.018717746582732057\n3\tclinton\t0.01677662526301099\n3\tjust\t0.013029174763779383\n3\tpeople\t0.010207181931985578\n"}]},"apps":[],"jobName":"paragraph_1495313281679_-701494720","id":"20170302-205337_768402024","dateCreated":"2017-05-20T15:48:01-0500","dateStarted":"2017-05-20T16:20:16-0500","dateFinished":"2017-05-20T16:21:51-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12458"},{"title":"Sentiment Analsysis","text":"import org.apache.spark.sql.functions._\nimport com.databricks.spark.corenlp.functions._\nimport org.apache.spark.sql.types._\n//using Standford Core NLP for sentimental analysis \n\nval sentiment_output_path = z.angular(\"sentiment_output_path\").toString\n\ntweets.filter(\"description is not null\").select('id, sentiment('description).as('sentiment)).write.parquet(sentiment_output_path)\n\nval sentiments = spark.read.format(\"parquet\").load(sentiment_output_path)\n\nsentiments.createOrReplaceTempView(\"sentiments\")\n\nval ssdf = spark.sql(\"SELECT sentiment, count(1) as total FROM sentiments group by sentiment\")\n\nz.show(ssdf)\n//z.show(sentiments.describe(\"sentiment\"));","user":"xwj","dateUpdated":"2017-05-20T18:41:56-0500","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":469,"optionOpen":true,"setting":{"stackedAreaChart":{},"scatterChart":{"xAxis":{"name":"id","index":0,"aggr":"sum"},"yAxis":{"name":"sentiment","index":2,"aggr":"sum"}},"multiBarChart":{}},"keys":[{"name":"sentiment","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"total","index":1,"aggr":"sum"}],"commonSetting":{}},"helium":{}},"1":{"graph":{"mode":"table","height":207,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.functions._\n\nimport com.databricks.spark.corenlp.functions._\n\nimport org.apache.spark.sql.types._\n\nsentiment_output_path: String = ~/sentiment/result\n\nsentiments: org.apache.spark.sql.DataFrame = [id: string, sentiment: int]\n\nssdf: org.apache.spark.sql.DataFrame = [sentiment: int, total: bigint]\n"},{"type":"TABLE","data":"sentiment\ttotal\n1\t76909\n3\t7218\n4\t626\n2\t14672\n0\t557\n"}]},"apps":[],"jobName":"paragraph_1495313281679_-701494720","id":"20170302-165232_309595451","dateCreated":"2017-05-20T15:48:01-0500","dateStarted":"2017-05-20T16:34:32-0500","dateFinished":"2017-05-20T16:44:59-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12460"},{"title":"Sentiment Analysis with Training Data using Logistic Regression","text":"import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, StopWordsRemover}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\n\nval training_file= z.angular(\"senti_train_path\").toString\n\nval training = spark.read.format(\"csv\").option(\"header\", true).load(training_file).selectExpr(\"description\", \"human_senti + 0.0 as label\")\n\n//val training= coded_tweets.sample(false, 0.8)\n//val test= coded_tweets.except(training)\nval test = tweets.select('id, 'description).filter(\"description is not null\")\n//training.show\n//test.show\n\nval tokenizer = new RegexTokenizer()\n  .setGaps(false)\n  .setPattern(\"\\\\p{L}+\")\n  .setInputCol(\"description\")\n  .setOutputCol(\"words\")\n  \nval stopwords: Array[String] = sc.textFile(stopWords_path).flatMap(_.stripMargin.split(\"\\\\s+\")).collect ++ Array(\"rt\")\n\nval filterer = new StopWordsRemover()\n  .setStopWords(stopwords)\n  .setCaseSensitive(false)\n  .setInputCol(\"words\")\n  .setOutputCol(\"filtered\")\n  \n val countVectorizer = new CountVectorizer()\n  .setInputCol(\"filtered\")\n  .setOutputCol(\"features\")\n \n val lr = new LogisticRegression()\n  .setMaxIter(10)\n  .setRegParam(0.2)\n  .setElasticNetParam(0.0)\n  \n  val pipeline = new Pipeline().setStages(Array(tokenizer, filterer, countVectorizer, lr))\n  \n  val lrModel = pipeline.fit(training)\n  \n  lrModel.transform(test).select('id, 'description, 'prediction).write.mode(\"overwrite\").option(\"header\", true).csv(sentiment_output_path)\n\nval sentiments = spark.read.format(\"csv\").option(\"header\", true).load(sentiment_output_path)\n\nsentiments.createOrReplaceTempView(\"sentiments\")\n\nval ssdf = spark.sql(\"SELECT prediction, count(1) as total FROM sentiments group by prediction\")\n\nz.show(ssdf)\n//result.registerTempTable(\"preds\");\n//z.show(result)\n","user":"xwj","dateUpdated":"2017-05-20T18:42:17-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, StopWordsRemover}\n\nimport org.apache.spark.sql.functions._\n\nimport org.apache.spark.ml.classification.LogisticRegression\n\nimport org.apache.spark.ml.Pipeline\n\ntraining_file: String = /data/projects/G-818781/data/trump/coded/2017_random_coded.csv\n\ntraining: org.apache.spark.sql.DataFrame = [description: string, label: double]\n\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, description: string]\n\ntokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_adece3916276\nstopwords: Array[String] = Array(a, about, above, across, after, afterwards, again, against, all, almost, alone, along, already, also, although, always, am, among, amongst, amoungst, amount, an, and, another, any, anyhow, anyone, anything, anyway, anywhere, are, around, as, at, back, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, below, beside, besides, between, beyond, bill, both, bottom, but, by, call, can, cannot, cant, co, computer, con, could, couldnt, cry, de, describe, detail, do, done, down, due, during, each, eg, eight, either, eleven, else, elsewhere, empty, enough, etc, even, ever, every, everyone, everything, everywhere, except, few, fifteen, fify, fill, find, fire, first, five, for, former, formerly, forty, found, four, from, front,...\nfilterer: org.apache.spark.ml.feature.StopWordsRemover = stopWords_f0f11dcb1af4\n\ncountVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_bdf46b16e6e8\n\nlr: org.apache.spark.ml.classification.LogisticRegression = logreg_07a32be3b132\n\npipeline: org.apache.spark.ml.Pipeline = pipeline_ce12bd9a6266\n\nlrModel: org.apache.spark.ml.PipelineModel = pipeline_ce12bd9a6266\n\nsentiments: org.apache.spark.sql.DataFrame = [id: string, description: string ... 1 more field]\n\nssdf: org.apache.spark.sql.DataFrame = [prediction: string, total: bigint]\n"},{"type":"TABLE","data":"prediction\ttotal\n1.0\t39620\n0.0\t105\n2.0\t4270\n3.0\t20\n"}]},"apps":[],"jobName":"paragraph_1495319881625_-238905437","id":"20170520-173801_1620357410","dateCreated":"2017-05-20T17:38:01-0500","dateStarted":"2017-05-20T18:39:13-0500","dateFinished":"2017-05-20T18:39:49-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12461"},{"text":"","dateUpdated":"2017-05-20T15:48:01-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1495313281687_-692260746","id":"20170410-153743_1753060846","dateCreated":"2017-05-20T15:48:01-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12469"}],"name":"TwitterAnalysis","id":"2CJJCR3AW","angularObjects":{"2CJHPM7JW:shared_process":[{"name":"sentiment_output_path","object":"~/sentiment/result","noteId":"2CJJCR3AW","paragraphId":"20170302-165232_309595451"},{"name":"sentiment_output_path2","object":"sentiment/result2","noteId":"2CJJCR3AW","paragraphId":"20170520-173801_1620357410"},{"name":"sentiment_output_path","object":"/data/projects/G-818781/data/trump/coded","noteId":"2CJJCR3AW","paragraphId":"20170520-173801_1620357410"},{"name":"senti_train_path","object":"/data/projects/G-818781/data/trump/coded/2017_random_coded.csv","noteId":"2CJJCR3AW","paragraphId":"20170520-173801_1620357410"},{"name":"input_f","object":{"inputfile":"/data/projects/G-818781/data/trump/csv/2016/trump1.csv","from":"2016-01-19 15:00:00","end":"2018-01-19 20:00:00","stopwords_path":"/data/projects/G-818781/data/trump/stopwords.txt"},"noteId":"2CJJCR3AW","paragraphId":"20170302-163418_715093436"},{"name":"term_per_topic","object":"4","noteId":"2CJJCR3AW","paragraphId":"20170302-205337_768402024"},{"name":"num_topics","object":"4","noteId":"2CJJCR3AW","paragraphId":"20170302-205337_768402024"},{"name":"stop_words","object":2,"noteId":"2CJJCR3AW","paragraphId":"20170302-205317_1581000322"},{"name":"term_per_topic","object":10,"noteId":"2CJJCR3AW"},{"name":"input_f","object":"","noteId":"2CJJCR3AW"},{"name":"stop_words","object":2,"noteId":"2CJJCR3AW"},{"name":"num_topics","object":10,"noteId":"2CJJCR3AW"},{"name":"sentiment_output_path","object":"sentiment/result","noteId":"2CJJCR3AW"},{"name":"exclude_retweets","object":true,"noteId":"2CJJCR3AW"},{"name":"max_iter","object":100,"noteId":"2CJJCR3AW"}],"2CHP8FWZV:shared_process":[],"2CFG67JUJ:shared_process":[],"2CHS9ZCRV:shared_process":[],"2CHUXEH9Y:shared_process":[],"2CFEYRHYQ:shared_process":[],"2CHU7YAAQ:shared_process":[],"2CGE8QS4U:shared_process":[],"2CHU3XJUJ:shared_process":[],"2CGWY8HAF:shared_process":[],"2CJDX7U1S:shared_process":[],"2CGHE334U:shared_process":[],"2CFBHYKDX:shared_process":[],"2CHV7VES4:shared_process":[],"2CGBQTZ6Z:shared_process":[],"2CHWR56RW:shared_process":[],"2CEVYPMT8:shared_process":[],"2CH9DTZ2C:shared_process":[],"2CJK7E95K:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}